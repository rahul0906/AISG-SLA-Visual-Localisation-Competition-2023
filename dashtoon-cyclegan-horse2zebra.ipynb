{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1567596,"sourceType":"datasetVersion","datasetId":926321}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\n\nclass ImageFolderDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = [img for img in os.listdir(root_dir) if img.endswith(\".jpg\")]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.images[idx])\n        image = Image.open(img_name).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\n# Defining my datasets and dataloaders\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nsplit_size = 0.8\n\n# Defining my datasets and dataloaders\nhorse_dataset = ImageFolderDataset(root_dir='/kaggle/input/horse2zebra-dataset/trainA/', transform=transforms.ToTensor())\nzebra_dataset = ImageFolderDataset(root_dir='/kaggle/input/horse2zebra-dataset/trainB/', transform=transforms.ToTensor())\n\ntrain_size_horse = int(split_size * len(horse_dataset))\nval_size_horse = len(horse_dataset) - train_size_horse\ntrain_dataset_horse, val_dataset_horse = torch.utils.data.random_split(horse_dataset, [train_size_horse, val_size_horse])\n\ntrain_size_zebra = int(split_size * len(zebra_dataset))\nval_size_zebra = len(zebra_dataset) - train_size_zebra\ntrain_dataset_zebra, val_dataset_zebra = torch.utils.data.random_split(zebra_dataset, [train_size_zebra, val_size_zebra])\n\n\ntrain_horse_loader = DataLoader(train_dataset_horse, batch_size=1, shuffle=True, num_workers=2)\nval_horse_loader = DataLoader(val_dataset_horse, batch_size=1, shuffle=True, num_workers=2)\n\ntrain_zebra_loader = DataLoader(train_dataset_zebra, batch_size=1, shuffle=True, num_workers=2)\nval_zebra_loader = DataLoader(val_dataset_zebra, batch_size=1, shuffle=True, num_workers=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T08:35:10.734999Z","iopub.execute_input":"2023-11-27T08:35:10.735779Z","iopub.status.idle":"2023-11-27T08:35:10.758093Z","shell.execute_reply.started":"2023-11-27T08:35:10.735742Z","shell.execute_reply":"2023-11-27T08:35:10.757242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_images(real_A, real_B, fake_A, fake_B):\n    fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n    axes[0, 0].imshow(np.transpose(real_A, (1, 2, 0)))\n    axes[0, 0].set_title('Real Image A')\n    axes[0, 0].axis('off')\n\n    axes[0, 1].imshow(np.transpose(fake_B, (1, 2, 0)))\n    axes[0, 1].set_title('Generated Image B')\n    axes[0, 1].axis('off')\n\n    axes[1, 0].imshow(np.transpose(real_B, (1, 2, 0)))\n    axes[1, 0].set_title('Real Image B')\n    axes[1, 0].axis('off')\n\n    axes[1, 1].imshow(np.transpose(fake_A, (1, 2, 0)))\n    axes[1, 1].set_title('Generated Image A')\n    axes[1, 1].axis('off')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:35:10.762198Z","iopub.execute_input":"2023-11-27T08:35:10.762966Z","iopub.status.idle":"2023-11-27T08:35:10.772068Z","shell.execute_reply.started":"2023-11-27T08:35:10.762932Z","shell.execute_reply":"2023-11-27T08:35:10.771218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.inst_norm1 = nn.InstanceNorm2d(out_channels, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.inst_norm2 = nn.InstanceNorm2d(out_channels, affine=True)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.inst_norm1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.inst_norm2(out)\n        out += residual\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # Encoder\n        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3)\n        self.enc_inst_norm1 = nn.InstanceNorm2d(64, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.enc_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n        self.enc_inst_norm2 = nn.InstanceNorm2d(128, affine=True)\n\n        self.enc_conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n        self.enc_inst_norm3 = nn.InstanceNorm2d(256, affine=True)\n\n        # Residual blocks\n        self.residual_blocks = nn.Sequential(\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 256)\n        )\n\n        # Decoder\n        self.dec_conv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec_inst_norm1 = nn.InstanceNorm2d(128, affine=True)\n\n        self.dec_conv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec_inst_norm2 = nn.InstanceNorm2d(64, affine=True)\n\n        self.dec_conv3 = nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3)\n        self.dec_tanh = nn.Tanh()\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.relu(self.enc_inst_norm1(self.enc_conv1(x)))\n        enc2 = self.relu(self.enc_inst_norm2(self.enc_conv2(enc1)))\n        enc3 = self.relu(self.enc_inst_norm3(self.enc_conv3(enc2)))\n\n        # Residual blocks\n        residual = self.residual_blocks(enc3)\n\n        # Decoder\n        dec1 = self.relu(self.dec_inst_norm1(self.dec_conv1(residual)))\n        dec2 = self.relu(self.dec_inst_norm2(self.dec_conv2(dec1)))\n        output = self.dec_tanh(self.dec_conv3(dec2))\n\n        return output\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.inst_norm2 = nn.InstanceNorm2d(128, affine=True)\n\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n        self.inst_norm3 = nn.InstanceNorm2d(256, affine=True)\n\n        self.conv4 = nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n\n    def forward(self, x):\n        x = self.leaky_relu(self.conv1(x))\n        x = self.leaky_relu(self.inst_norm2(self.conv2(x)))\n        x = self.leaky_relu(self.inst_norm3(self.conv3(x)))\n        x = self.conv4(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:35:10.847365Z","iopub.execute_input":"2023-11-27T08:35:10.847612Z","iopub.status.idle":"2023-11-27T08:35:10.867807Z","shell.execute_reply.started":"2023-11-27T08:35:10.847591Z","shell.execute_reply":"2023-11-27T08:35:10.866960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize generators and discriminators\nimport itertools\nimport matplotlib.pyplot as plt\nG_horse2zebra = Generator()\nG_zebra2horse = Generator()\nD_horse = Discriminator()\nD_zebra = Discriminator()\n\n# Define optimizers for generators and discriminators\noptimizer_G = optim.Adam(itertools.chain(G_horse2zebra.parameters(), G_zebra2horse.parameters()), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D_horse = optim.Adam(D_horse.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D_zebra = optim.Adam(D_zebra.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Loss functions\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_cycle = nn.L1Loss()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:35:10.869451Z","iopub.execute_input":"2023-11-27T08:35:10.869763Z","iopub.status.idle":"2023-11-27T08:35:11.003916Z","shell.execute_reply.started":"2023-11-27T08:35:10.869734Z","shell.execute_reply":"2023-11-27T08:35:11.003167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\n# Training loop\nnum_epochs = 100\n\nfor epoch in range(num_epochs):\n    for horse_data, zebra_data in tqdm(zip(train_horse_loader, train_zebra_loader), total=len(train_horse_loader)):\n        # Training the discriminators\n        optimizer_D_horse.zero_grad()\n        optimizer_D_zebra.zero_grad()\n\n        # Forward pass through generators\n        fake_zebra = G_horse2zebra(horse_data)\n        fake_horse = G_zebra2horse(zebra_data)\n\n        # Adversarial loss\n        loss_D_horse = criterion_GAN(D_horse(horse_data), torch.zeros_like(D_horse(horse_data)))\n        loss_D_zebra = criterion_GAN(D_zebra(zebra_data), torch.zeros_like(D_zebra(zebra_data)))\n\n        loss_D_fake_horse = criterion_GAN(D_horse(fake_horse.detach()), torch.zeros_like(D_horse(fake_horse)))\n        loss_D_fake_zebra = criterion_GAN(D_zebra(fake_zebra.detach()), torch.zeros_like(D_zebra(fake_zebra)))\n\n        loss_D = 0.5 * (loss_D_horse + loss_D_fake_horse) + 0.5 * (loss_D_zebra + loss_D_fake_zebra)\n        loss_D.backward()\n        optimizer_D_horse.step()\n        optimizer_D_zebra.step()\n\n        # Training the generators\n        optimizer_G.zero_grad()\n\n        # Adversarial loss\n        loss_G_horse = criterion_GAN(D_horse(fake_horse), torch.ones_like(D_horse(fake_horse)))\n        loss_G_zebra = criterion_GAN(D_zebra(fake_zebra), torch.ones_like(D_zebra(fake_zebra)))\n\n        # Cycle consistency loss\n        reconstructed_horse = G_zebra2horse(fake_zebra)\n        reconstructed_zebra = G_horse2zebra(fake_horse)\n\n        loss_cycle_horse = criterion_cycle(reconstructed_horse, horse_data)\n        loss_cycle_zebra = criterion_cycle(reconstructed_zebra, zebra_data)\n\n        loss_G = loss_G_horse + loss_G_zebra + 10 * (loss_cycle_horse + loss_cycle_zebra)\n        loss_G.backward()\n        optimizer_G.step()\n        \n    with torch.no_grad():\n        generated_images_A = []\n        generated_images_B = []\n        real_images_A = []\n        real_images_B = []\n\n        for val_data_A, val_data_B in tqdm(zip(val_horse_loader, val_zebra_loader), total=len(val_horse_loader)):\n            if len(generated_images_A) > 3:\n                break\n                \n            x_val_A = val_data_A\n            x_val_B = val_data_B\n\n            # Forward pass through the generators\n            fake_B = G_horse2zebra(x_val_A)\n            fake_A = G_zebra2horse(x_val_B)\n            \n            content_loss_A = criterion_cycle(G_zebra2horse(x_val_B), x_val_B)\n            content_loss_B = criterion_cycle(G_horse2zebra(x_val_A), x_val_A)\n            \n            print(f\"Content Loss A: {content_loss_A.item()}, Content Loss B: {content_loss_B.item()}\")\n\n            # Accumulate generated and real images for later visualization\n            generated_images_A.append(fake_B.cpu().detach().squeeze(0))\n            generated_images_B.append(fake_A.cpu().detach().squeeze(0))\n            real_images_A.append(x_val_A.cpu().detach().squeeze(0))\n            real_images_B.append(x_val_B.cpu().detach().squeeze(0))\n\n        # Visualize a sample of generated and real images\n        visualize_images(\n            real_A=real_images_A[0], \n            real_B=real_images_B[0], \n            fake_A=generated_images_A[0], \n            fake_B=generated_images_B[0]\n        )\n\n\n    # Print or log the losses for monitoring the training process\n    print(f\"Epoch [{epoch}/{num_epochs}] - Loss_G: {loss_G.item()} - Loss_D: {loss_D.item()}\")\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:35:11.005392Z","iopub.execute_input":"2023-11-27T08:35:11.005667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained models if needed\ntorch.save(G_horse2zebra.state_dict(), 'G_horse2zebra.pth')\ntorch.save(G_zebra2horse.state_dict(), 'G_zebra2horse.pth')\ntorch.save(D_horse.state_dict(), 'D_horse.pth')\ntorch.save(D_zebra.state_dict(), 'D_zebra.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}